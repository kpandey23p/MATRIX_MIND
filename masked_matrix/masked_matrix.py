# -*- coding: utf-8 -*-
"""masked_matrix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BsXKvCc-4SJK_t3VB4nO8-nKz27gKhzn
"""

!pip install scikit-learn pandas numpy scipy tqdm requests

import os
import gc
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
import json
import gzip
import time
import requests
from io import BytesIO
from tqdm import tqdm
from sklearn.decomposition import TruncatedSVD
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import normalize

class OptimizedAmazonRecommender:
    def __init__(self, n_components=50, alpha=0.01, test_size=0.1, chunk_size=50000):
        self.n_components = n_components  # Increased latent factors
        self.alpha = alpha  # Reduced regularization
        self.test_size = test_size
        self.chunk_size = chunk_size
        self.user_map = None
        self.product_map = None
        self.svd = None
        self.ridge = None

    def stream_jsonl(self, file_obj, max_lines=None):
        """Generator to stream JSONL data line by line"""
        count = 0
        for line in file_obj:
            if max_lines and count >= max_lines:
                break
            try:
                yield json.loads(line)
                count += 1
            except json.JSONDecodeError:
                continue

    def build_enhanced_matrix(self, url, local_filename="data.jsonl.gz"):
        """Build matrix with additional preprocessing"""
        if not os.path.exists(local_filename):
            print(f"Downloading {url}...")
            response = requests.get(url, stream=True)
            with open(local_filename, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

        # First pass: collect IDs and rating stats
        print("Analyzing data...")
        users = set()
        products = set()
        ratings = []

        with gzip.open(local_filename, 'rt') as f:
            for record in tqdm(self.stream_jsonl(f), desc="Scanning"):
                try:
                    users.add(record['user_id'])
                    products.add(record['asin'])
                    ratings.append(float(record['rating']))
                except (KeyError, ValueError):
                    continue

        self.user_map = {u: i for i, u in enumerate(users)}
        self.product_map = {p: i for i, p in enumerate(products)}

        # Calculate global average rating
        self.global_avg = np.mean(ratings) if ratings else 3.0
        del users, products, ratings
        gc.collect()

        # Second pass: build matrix with mean-centered ratings
        print("Building enhanced matrix...")
        row_ind = []
        col_ind = []
        data = []

        with gzip.open(local_filename, 'rt') as f:
            for record in tqdm(self.stream_jsonl(f), desc="Processing"):
                try:
                    row_ind.append(self.user_map[record['user_id']])
                    col_ind.append(self.product_map[record['asin']])
                    # Mean-center ratings
                    data.append(float(record['rating']) - self.global_avg)
                except (KeyError, ValueError):
                    continue

        matrix = csr_matrix((data, (row_ind, col_ind)),
                          shape=(len(self.user_map), len(self.product_map)),
                          dtype=np.float32)

        # Normalize by user ratings
        matrix = normalize(matrix, norm='l2', axis=1)

        print(f"Final matrix: {matrix.shape}, sparsity: {1-matrix.nnz/(matrix.shape[0]*matrix.shape[1]):.2%}")
        return matrix

    def improved_train_test_split(self, matrix):
        """Stratified split by users"""
        print("Creating stratified split...")
        train = matrix.copy()
        test = matrix.copy()

        # Ensure each user has at least one test rating
        user_counts = np.diff(matrix.indptr)
        mask = np.zeros(matrix.nnz, dtype=bool)

        for i in range(matrix.shape[0]):
            if user_counts[i] > 1:  # Users with multiple ratings
                user_ratings = slice(matrix.indptr[i], matrix.indptr[i+1])
                n_test = max(1, int(user_counts[i] * self.test_size))
                user_mask = np.zeros(user_counts[i], dtype=bool)
                user_mask[:n_test] = True
                np.random.shuffle(user_mask)
                mask[user_ratings] = user_mask

        train.data[mask] = 0
        test.data[~mask] = 0

        train.eliminate_zeros()
        test.eliminate_zeros()

        return train, test

    def train_enhanced_model(self, train):
        """Enhanced training with better feature engineering"""
        print("Training enhanced model...")

        # Use randomized SVD with more iterations
        self.svd = TruncatedSVD(n_components=self.n_components,
                              n_iter=7,
                              random_state=42)

        U = self.svd.fit_transform(train)
        sigma = np.diag(self.svd.singular_values_)
        Vt = self.svd.components_

        # Add user and item biases
        rows, cols = train.nonzero()
        ratings = train[rows, cols].A1 + self.global_avg  # Add back global mean

        # Enhanced features
        X = np.hstack([
            U[rows],
            (sigma @ Vt).T[cols],
            np.ones((len(rows), 1))  # Bias term
        ])

        self.ridge = Ridge(alpha=0.3, fit_intercept=False)
        self.ridge.fit(X, ratings)

        # Store user and item factors separately
        self.user_factors = U
        self.item_factors = (sigma @ Vt).T

    def evaluate_enhanced(self, train, test):
        """Improved evaluation with better prediction handling"""
        print("Running enhanced evaluation...")
        rows, cols = test.nonzero()
        actual = test[rows, cols].A1 + self.global_avg

        # Predict in batches with bias terms
        predictions = []
        batch_size = min(50000, len(rows))

        for i in tqdm(range(0, len(rows), batch_size), desc="Predicting"):
            batch_rows = rows[i:i+batch_size]
            batch_cols = cols[i:i+batch_size]

            features = np.hstack([
                self.user_factors[batch_rows],
                self.item_factors[batch_cols],
                np.ones((len(batch_rows), 1))
            ])
            predictions.extend(self.ridge.predict(features))

        predictions = np.clip(predictions, 1, 5)  # Clip to rating range

        mse = mean_squared_error(actual, predictions)

        # Improved hit ratio calculation
        hit_window = 0.7  # Increased from 0.5 to account for rating scale
        hit_ratio = np.mean(np.abs(np.array(predictions) - actual) <= hit_window)

        # Additional metrics
        accuracy = np.mean(np.round(predictions) == np.round(actual))

        print(f"\nEnhanced Evaluation Results:")
        print(f"- MSE: {mse:.4f}")
        print(f"- Hit Ratio (±{hit_window}): {hit_ratio:.4f}")
        print(f"- Exact Match Accuracy: {accuracy:.4f}")

        # Show better samples
        print("\nSample Predictions:")
        sample_indices = np.random.choice(len(actual), size=min(10, len(actual)), replace=False)
        for idx in sample_indices:
            print(f"User {rows[idx]}, Item {cols[idx]}:")
            print(f"  Actual: {actual[idx]:.1f} → Predicted: {predictions[idx]:.1f}")

if __name__ == "__main__":
    # Initialize with optimized parameters
    recommender = OptimizedAmazonRecommender(
        n_components=40,
        alpha=0.1,  # Lower regularization
        test_size=0.1,
        chunk_size=50000
    )

    # Example URL - replace with your actual data
    data_url = "https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Appliances.jsonl.gz"

    print("="*50)
    print("STEP 1: Loading and preprocessing data")
    print("="*50)
    matrix = recommender.build_enhanced_matrix(data_url)

    print("\n" + "="*50)
    print("STEP 2: Creating stratified train/test split")
    print("="*50)
    train, test = recommender.improved_train_test_split(matrix)
    del matrix
    gc.collect()

    print("\n" + "="*50)
    print("STEP 3: Training enhanced model")
    print("="*50)
    recommender.train_enhanced_model(train)

    print("\n" + "="*50)
    print("STEP 4: Running enhanced evaluation")
    print("="*50)
    recommender.evaluate_enhanced(train, test)